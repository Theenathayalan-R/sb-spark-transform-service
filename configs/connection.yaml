# Connection-only configuration
starburst:
  host: "your-starburst-host.com"
  port: 443
  catalog: "your_catalog"
  schema: "your_schema"
  user: "${STARBURST_USER}"
  password: "${STARBURST_PASSWORD}"
  ssl: true
  application_name: "transformation_pipeline"
  fetchsize: 50000  # Increased for better throughput
  batchsize: 50000  # Increased for better throughput
  connection_pool_size: 10  # Connection pooling
  max_retries: 3
  retry_backoff_seconds: 2.0
  log_counts: false  # Disable for large datasets to avoid performance hits

spark:
  app_name: "DataTransformationPipeline"
  jdbc_jar_path: "/path/to/trino-jdbc-428.jar"
  configs:
    # Adaptive Query Execution
    spark.sql.adaptive.enabled: "true"
    spark.sql.adaptive.coalescePartitions.enabled: "true"
    spark.sql.adaptive.advisoryPartitionSizeInBytes: "128MB"
    spark.sql.adaptive.skewJoin.enabled: "true"
    spark.sql.adaptive.localShuffleReader.enabled: "true"
    
    # Memory and Performance
    spark.executor.memory: "8g"
    spark.executor.memoryFraction: "0.8"
    spark.executor.cores: "4"
    spark.driver.memory: "4g"
    spark.driver.maxResultSize: "2g"
    
    # Dynamic Allocation
    spark.dynamicAllocation.enabled: "true"
    spark.dynamicAllocation.minExecutors: "2"
    spark.dynamicAllocation.maxExecutors: "20"
    spark.dynamicAllocation.initialExecutors: "4"
    
    # Shuffle and Partitioning
    spark.sql.shuffle.partitions: "200"
    spark.sql.adaptive.shuffle.targetPostShuffleInputSize: "128MB"
    
    # Serialization and Arrow
    spark.serializer: "org.apache.spark.serializer.KryoSerializer"
    spark.sql.execution.arrow.pyspark.enabled: "true"
    spark.sql.execution.arrow.pyspark.fallback.enabled: "true"
    
    # Caching and Storage
    spark.sql.cache.serializer: "org.apache.spark.sql.execution.columnar.InMemoryTableScanExec"
    spark.storage.level: "MEMORY_AND_DISK_SER"
    
    # Network and I/O
    spark.network.timeout: "800s"
    spark.sql.broadcastTimeout: "36000"
